{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# What is our aim?\n# We are building context encoder \n# It has one encoder, followed by a channel based fully connected layer (to decrease parameters \n# and bring account distant features), and then followed by decoder (which just reverse of encoder \n# transpose coonvolutions)\n# This is described above is generator which will have a L2 loss and now we will describe a discriminator\n# A discriminator is something that gives output as 1 for real images and 0 for fake images\n# and we are basically trying to fool our discriminator that is filling our void so visually pleasingly such that\n# the disrciminator considers it real and we have an associated adversial loss with it","metadata":{"_uuid":"6f893dc1-6ade-4f13-a98a-2b6220c37e2f","_cell_guid":"7cc773c1-a035-4742-8110-625458585db5","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#imports\nimport os\nimport random\nimport pathlib\nimport tensorflow as tf\nfrom tensorflow.keras.utils import Sequence\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm import tqdm\nfrom itertools import chain\nfrom skimage.io import imread, imshow, imread_collection, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.morphology import label\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import activations\n\n\n\nfrom tensorflow.keras.layers import ZeroPadding2D\nfrom keras.models import Model, load_model\nfrom keras.layers import Input, Dense\nfrom keras.layers.core import Dropout, Lambda\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.layers import Cropping2D\nfrom tensorflow.keras import activations\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras import regularizers\nfrom keras import backend as K\nimport cv2\nfrom matplotlib import image as matimg, pyplot\nimport tensorflow as tf\nfrom PIL import Image as im\nfrom tensorflow.keras.layers import Layer\nfrom tensorflow.keras.optimizers import Adam\nimport pickle\nimport keras.backend as K","metadata":{"_uuid":"3f79fdef-3227-4aec-8318-830af29c23cb","_cell_guid":"a1b3edce-bca0-4430-bdee-c6df4f8ea70d","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from __future__ import print_function, division\nfrom keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise\nfrom keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.layers.convolutional import UpSampling2D, Conv2D\nfrom keras.models import Sequential, Model\nfrom keras.optimizers import Adam\nfrom keras import losses\nfrom keras.utils import to_categorical\nimport keras.backend as K\n\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport os","metadata":{"_uuid":"b7293c9f-bb62-4d6f-b225-129511091dae","_cell_guid":"181a698b-e044-4140-ba24-dee98b34a664","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Image dataset CIFAR 10\npath = open('../input/cifar10-preprocessed/data.pickle','rb')\ndata = pickle.load(path)\npath.close","metadata":{"_uuid":"68f0e934-b005-46d3-b107-832aef892c49","_cell_guid":"980e2aaf-e8e2-4aa6-97d3-514463c18623","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Image Dataset CIFAR 10\nprint(data.keys())\nx_train = data['x_train']\ny_train = data['y_train']\ny_test = data['y_test']\nx_validation = data['x_validation']\nx_test = data['x_test']\ny_validation = data['y_validation']","metadata":{"_uuid":"f66a3b77-12aa-4fb3-9414-13d63214f5e6","_cell_guid":"4d844096-879f-41db-ab97-163a31341d5d","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Image Dataset CIFAR 10\nX_deer = x_train[(y_train==4).flatten()]\nX_horse = x_train[(y_train==6).flatten()]\n[R,G,B] = X_deer[13]\nimg = np.dstack((R,G,B))\nplt.imshow(img)","metadata":{"_uuid":"481ba996-f393-4f8a-8123-725f18e4fe93","_cell_guid":"975e1e39-85e7-4a45-a418-365fff59b9e3","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ImagenetMini Dataset\nX_train = np.zeros((4000,256,256,3))\ni = 0;\nfor dirname, _, filenames in os.walk('/kaggle/input/imagenetmini-1000/imagenet-mini/train'):\n        for filename in filenames:\n#             print(os.path.join(dirname, filename))\n            img = np.array(im.open(os.path.join(dirname, filename)))\n            if(img.shape[0]>=256 and img.shape[1]>=256 and len(img.shape) == 3):\n#                 print(img.shape)\n                print(i)\n                X_train[i,:,:,:]=img[0:256,0:256,:]\n                i=i+1\n                if(i==4000):\n                    break\n            if(i==4000):\n                break\n        if(i==4000):\n            break\n#             X_train[i]=np.array(im.open(os.path.join(dirname, filename))))","metadata":{"_uuid":"8748e68e-c8b1-47fc-9d09-aca2e48d2470","_cell_guid":"d6ed7502-ded0-44e6-8dab-23902b8897ad","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Rescale -1 to 1 image intensities\nX_train = X_train/127.5 -1","metadata":{"_uuid":"443fe2bc-4863-4baf-b8b2-ea500467eb22","_cell_guid":"3c40dff4-b7a9-4268-a91c-47e118542df9","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numrows = 256\nnumcols = 256\ncolor = 3\nmask_height = 64\nmask_width = 64\nImshape = (numrows,numcols,color)\nmaskShape = (mask_height,mask_width,color)\noptimizer = Adam(0.0001)","metadata":{"_uuid":"5ea7aca9-b3a2-475b-9c85-85136bc92c82","_cell_guid":"2b23a45a-8388-48cf-8ecd-c0b389916593","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modelgen = Sequential()\n#           (256x256x3)\nmodelgen.add(Conv2D(64, kernel_size=3, strides=2, input_shape=Imshape, padding=\"same\"))\nmodelgen.add(LeakyReLU(alpha=0.2))\nmodelgen.add(BatchNormalization(momentum=0.8))\n#           (128x128x64)\nmodelgen.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\nmodelgen.add(LeakyReLU(alpha=0.2))\nmodelgen.add(BatchNormalization(momentum=0.8))\n#           (64x64x64)\nmodelgen.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\nmodelgen.add(LeakyReLU(alpha=0.2))\nmodelgen.add(BatchNormalization(momentum=0.8))\n#           (32x32x64)\nmodelgen.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\nmodelgen.add(LeakyReLU(alpha=0.2))\nmodelgen.add(BatchNormalization(momentum=0.8))\n#           (16x16x128)\nmodelgen.add(Conv2D(256, kernel_size=3, strides=2, padding=\"same\"))\nmodelgen.add(LeakyReLU(alpha=0.2))\nmodelgen.add(BatchNormalization(momentum=0.8))\n#           (8x8x256)\nmodelgen.add(Conv2D(512, kernel_size=3, strides=2, padding=\"same\"))\nmodelgen.add(LeakyReLU(alpha=0.2))\nmodelgen.add(Dropout(0.5))\n#           (4x4x512)\n\n#Decoder\nmodelgen.add(UpSampling2D())\n#           (8x8x512)\nmodelgen.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n#           (8x8x128)\nmodelgen.add(Activation('relu'))\nmodelgen.add(BatchNormalization(momentum=0.8))\nmodelgen.add(UpSampling2D())\n#           (16x16x128)\nmodelgen.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n#           (16x16x64)\nmodelgen.add(Activation('relu'))\nmodelgen.add(BatchNormalization(momentum=0.8))\nmodelgen.add(UpSampling2D())\n#         (32x32x64)\nmodelgen.add(Conv2D(32, kernel_size=3, padding=\"same\"))\n#           (32x32x32)\nmodelgen.add(Activation('relu'))\nmodelgen.add(BatchNormalization(momentum=0.8))\nmodelgen.add(UpSampling2D())\n#         (64x64x32)\nmodelgen.add(Conv2D(color, kernel_size=3, padding=\"same\"))\n#           (16x16x3)\nmodelgen.add(Activation('tanh'))\n\nmodelgen.summary()\n\ninput_img = Input(shape=Imshape)\nmissing = modelgen(input_img)\ngenerator = Model(input_img,missing)","metadata":{"_uuid":"3ce63534-4838-4025-94b2-f593a78c88c9","_cell_guid":"ef77a4ee-db76-4ae0-85b2-31a61e224052","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Discriminator\nmodel = Sequential()\n#           (64x64x3)\nmodel.add(Conv2D(64, kernel_size=3, strides=2, input_shape=maskShape, padding=\"same\"))\nmodel.add(LeakyReLU(alpha=0.2))\nmodel.add(BatchNormalization(momentum=0.8))\n#           (32x32x32)\nmodel.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\nmodel.add(LeakyReLU(alpha=0.2))\nmodel.add(BatchNormalization(momentum=0.8))\n#           (16x16x64)\nmodel.add(Conv2D(256, kernel_size=3, strides=2, padding=\"same\"))\nmodel.add(LeakyReLU(alpha=0.2))\nmodel.add(BatchNormalization(momentum=0.8))\n#           (8x8x128)\nmodel.add(Conv2D(512, kernel_size=3, strides=2, padding=\"same\"))\nmodel.add(LeakyReLU(alpha=0.2))\nmodel.add(BatchNormalization(momentum=0.8))\n#           (4x4x256)\nmodel.add(Conv2D(512, kernel_size=3, strides=2, padding=\"same\"))\nmodel.add(LeakyReLU(alpha=0.2))\nmodel.add(BatchNormalization(momentum=0.8))\n#           (2x2x512)\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()\n\nmissing = Input(shape=maskShape)\nvalid = model(missing)\ndiscriminator = Model(missing,valid)","metadata":{"_uuid":"72917cf9-23dc-4b31-a083-c977e2723f4d","_cell_guid":"0c4efd22-7203-45fd-934d-11b302915f9f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def maskImages(imgs):\n        lowerY = np.random.randint(0,numrows -mask_height,imgs.shape[0])\n        upperY = lowerY + mask_height\n        lowerX = np.random.randint(0,numcols -mask_width,imgs.shape[0])\n        upperX = lowerX + mask_width\n\n        inputImages = np.empty_like(imgs)\n        missingParts = np.empty((imgs.shape[0],mask_height,mask_width,3))\n        for i in range(imgs.shape[0]):\n            maskedImage = imgs[i].copy()\n            missingParts[i] = maskedImage[lowerY[i]:upperY[i],lowerX[i]:upperX[i],:].copy()\n            maskedImage[lowerY[i]:upperY[i],lowerX[i]:upperX[i],:] = 0\n            inputImages[i] = maskedImage\n\n        return inputImages, missingParts, (lowerY,upperY,lowerX,upperX)","metadata":{"_uuid":"292c1d89-d799-4f90-afd6-864ca841a2ea","_cell_guid":"ea522771-84ba-4ae9-95ba-c2625eb36126","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"discriminator.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])\ninput_img = Input(shape=Imshape)\nmissing = generator(input_img)\ndiscriminator.trainable = False\nvalid = discriminator(missing)\ncombined = Model(inputs = input_img, outputs = [missing,valid])\ncombined.compile(loss=['mse','binary_crossentropy'], loss_weights = [0.999,0.001], optimizer=optimizer)","metadata":{"_uuid":"b4c69cc5-75be-438d-bc60-1c5517abdcd7","_cell_guid":"d0aa9124-d6cd-47d0-b7ae-c7bca6a8364f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sample_images(epoch, imgs):\n        r, c = 3, 3\n\n        inputImages, missingParts, (y1, y2, x1, x2) = maskImages(imgs)\n        generatorPredicted = generator.predict(inputImages)\n\n#                 to bring range back to (0,1)\n        imgs = 0.5 * imgs + 0.5\n        inputImages = 0.5 * inputImages + 0.5\n        generatorPredicted = 0.5 * generatorPredicted + 0.5\n\n        fig, axs = plt.subplots(r, c)\n        for i in range(c):\n            axs[0,i].imshow(imgs[i, :,:])\n            axs[0,i].axis('off')\n            axs[1,i].imshow(inputImages[i, :,:])\n            axs[1,i].axis('off')\n            filled_in = imgs[i].copy()\n            filled_in[y1[i]:y2[i], x1[i]:x2[i], :] = generatorPredicted[i]\n            axs[2,i].imshow(filled_in)\n            axs[2,i].axis('off')\n        fig.savefig(\"%d.png\" % epoch)\n        plt.close()","metadata":{"_uuid":"2200a92d-9e4f-4e96-b574-7e13c0d61658","_cell_guid":"7e83c5ab-a26b-4300-9798-15c9d278da24","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(epochs,batch_size=128, sample_interval=50):\n\n        valid = np.ones((batch_size,1))\n        fake = np.zeros((batch_size,1))\n\n        for epoch in range(epochs):\n            ids = np.random.randint(0,4000,batch_size)\n            imgs = X_train[ids]\n\n            inputImages, missingParts, _ = maskImages(imgs)\n\n            generatorPredicted = generator.predict(inputImages)\n\n            d_loss_real = discriminator.train_on_batch(missingParts,valid)\n            d_loss_fake = discriminator.train_on_batch(generatorPredicted,fake)\n            d_loss = 0.5*np.add(d_loss_fake,d_loss_real)\n\n            g_loss = combined.train_on_batch(inputImages, [missingParts,valid])\n\n            print (\"%d [D loss: %f, acc: %.2f%%] [G loss: %f, mse: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss[0], g_loss[1]))\n\n            if epoch % sample_interval == 0:\n                ids = np.random.randint(0,4000,3)\n                imgs = X_train[ids]\n                sample_images(epoch,imgs)","metadata":{"_uuid":"5d9c1194-4cd9-4403-912c-ab4892ff0d8b","_cell_guid":"505758cd-60ca-4b17-8fdc-b1b6d099996a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train(epochs=2000, batch_size=64, sample_interval = 50)","metadata":{"_uuid":"453fe8e5-005c-440d-ad05-931549474f57","_cell_guid":"af889d0c-60fe-4159-9cc0-3bb8e638aa81","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trial = 990000\nfor i in range(0,3999,3):\n    sample_images(trial+i,X_train[i:i+3])","metadata":{"_uuid":"fb3baff2-ea32-4c80-8233-ad8d549a7260","_cell_guid":"04b2a784-dcf4-4b1b-b1db-80f887853781","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"afd8cad6-7109-4541-91b9-08f6c915f055","_cell_guid":"7eb62bb1-df47-43c1-b9bc-fa2a14d36de3","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"111222cb-4d03-435b-9618-b94cc184bcc4","_cell_guid":"ae2f9aff-84da-4c1a-9b92-3e826bbd6523","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"4e5b0175-fa2a-40af-a87a-5f7987fd488b","_cell_guid":"d649d1ca-a9d8-4c15-b2e1-0f6fead0b429","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}